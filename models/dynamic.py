import torch 
from torch.distributed import rpc 

from .framework import Euler_Embed_Unit, Euler_Encoder, Euler_Recurrent
from .utils import _remote_method, _remote_method_async

class DynamicEncoder(Euler_Encoder):
    '''
    Dynamic implimentation of Euler_Encoder interface
    '''

    def __init__(self, module: Euler_Embed_Unit, head: bool, **kwargs):
        '''
        Constructor for DynamicEncoder

        parameters 
        ----------
        module : Euler_Embed_Unit
            The model to encode temporal data. module.forward must accept an enum 
            reprsenting train/val/test and nothing else. See embedders.py for acceptable
            modules 
        head : boolean 
            If this worker holds timestep 0, which will never be encoded, as dynamic modules
            aim to predict future snapshots, it needs to know not to run loss on snapshot[0]
        kwargs : dict
            any args for the DDP constructor

        '''
        super().__init__(module, **kwargs)
        self.is_head = 1 if head else 0
        if self.is_head:
            print("%s is head" % rpc.get_worker_info().name)


    def decode_all(self, zs):
        '''
            Given node embeddings, return edge likelihoods for 
        all subgraphs held by this model
            For dynamic model, assume we are given embeddings 
        for timesteps -1 to N-1 (where Z_{-1} is a dummy value
        to be ignored if this is worker 0) with which to predict
        E_1 to E_N

        zs : torch.Tensor
            A T x d x N tensor of node embeddings generated by the models, 
            it is safe to assume z[n] are the embeddings for nodes in the 
            snapshot held by this model's TGraph at timestep n
        '''
        preds = []
        for i in range(self.module.data.T-self.is_head):
            preds.append(
                self.decode(self.module.data.eis[i+self.is_head], zs[i+self.is_head])
            )

        return preds

    
    def score_edges(self, z, partition, nratio):
        '''
        Given a set of Z embeddings, returns likelihood scores for all known
        edges, and randomly sampled negative edges

        z : torch.Tensor
            A T x d x N tensor of node embeddings generated by the models, 
            it is safe to assume z[n] are the embeddings for nodes in the 
            snapshot held by this model's TGraph at timestep n
        partition : int 
            An enum representing if this is training/validation/testing for 
            generating negative edges 
        nratio : float
            The model samples nratio * |E| negative edges for calculating loss
        '''
        
        # Skip neg edges for E_0 if this is the head node
        n = self.module.data.get_negative_edges(
            partition, nratio=nratio, start=self.is_head
        )

        p_scores = []
        n_scores = []

        # Head worker is given a dummy Z_{-1} to be skipped so
        # Z_0 is aligned with E_1 to be used for prediction
        for i in range(self.is_head, len(z)):
            p = self.module.data.ei_masked(partition, i)
            if p.size(0) == 0:
                continue

            p_scores.append(self.decode(p, z[i]))
            n_scores.append(self.decode(n[i-self.is_head], z[i]))

        p_scores = torch.cat(p_scores, dim=0)
        n_scores = torch.cat(n_scores, dim=0)

        return p_scores, n_scores


    def calc_loss(self, z, partition, nratio):
        '''
        Sum up all of the loss per time step, then average it. For some reason
        this works better than running score edges on everything at once. It's better
        to run BCE per time step rather than all at once

        z : torch.Tensor
            A T x d x N tensor of node embeddings generated by the models, 
            it is safe to assume z[n] are the embeddings for nodes in the 
            snapshot held by this model's TGraph at timestep n
        partition : int 
            An enum representing if this is training/validation/testing for 
            generating negative edges 
        nratio : float
            The model samples nratio * |E| negative edges for calculating loss
        '''
        tot_loss = torch.zeros(1)
        ns = self.module.data.get_negative_edges(
            partition, nratio=nratio, start=self.is_head
        )

        for i in range(self.is_head, len(z)):

            # Edge case. Prevents nan errors when not enough edges
            # only happens with very small timewindows 
            ps = self.module.data.ei_masked(partition, i)
            if ps.size(1) == 0:
                continue

            tot_loss += self.bce(
                self.decode(ps, z[i]),
                self.decode(ns[i-self.is_head], z[i])
            )

        return tot_loss.true_divide(len(z))


class DynamicRecurrent(Euler_Recurrent):
    '''
        With very minimal changes to the static version, we can make the 
    module work for predictive tasks as well. 
        The main changes involve which embeddings we send to 
    which workers, as they are now trying to predict future time steps.
    As such, if a worker holds edge lists at times t-t*n we send it embeddings
    generated for steps t-1 to t*n-1 
        In the edge case of the worker holding timestep 0 (which would recieve an 
    embedding for -1 which doesn't exist), we provide a dummy -1 embedding that
    that worker knows to ignore
    '''

    def score_all(self, zs):
        '''
        Has the distributed models score and label all of their edges
        For dynamic, we append a dummy Z_{-1} value that's ignored in 
        the workers, to align embeds with future edge lists 

        zs : torch.Tensor 
            A T x d x N tensor of node embeddings generated by each graph snapshot
            Need to offset according to how far in the future embeddings are supposed
            to represent.
        '''
        zs = torch.cat(
            [torch.zeros(zs[0].size()).unsqueeze(0), zs]
        )
        
        futs = []
        start = 0
    
        for i in range(self.num_workers):
            end = start + self.len_from_each[i]
            futs.append(
                _remote_method_async(
                    DynamicEncoder.decode_all,
                    self.gcns[i],
                    zs[start : end]
                )
            )
            start = end 

        scores = [f.wait() for f in futs]
        ys = [
            _remote_method(
                DynamicEncoder.get_data_field,
                self.gcns[i],
                'ys'
            ) for i in range(self.num_workers)
        ]

        # Remove labels for edgelist 0 as it has no embeddings 
        # it can be compared to 
        ys[0] = ys[0][1:]
        return scores, ys



    def loss_fn(self, zs, partition, nratio=1):
        '''
        Runs NLL on each worker machine given the generated embeds
        Need to change which zs are given to workers depending on if 
        predictive or static 
        For dynamic, only difference is the dummy Z_{-1} value

        zs : torch.Tensor 
            A T x d x N tensor of node embeddings generated by each graph snapshot
            Need to offset according to how far in the future embeddings are supposed
            to represent.
        partition : int
            enum representing train, validation, test sent to workers
        nratio : float
            The workers sample nratio * |E| negative edges for calculating loss
        '''
        
        zs = torch.cat(
            [torch.zeros(zs[0].size()).unsqueeze(0), zs]
        )
        
        futs = []
        start = 0
    
        for i in range(self.num_workers):
            end = start + self.len_from_each[i]
            futs.append(
                _remote_method_async(
                    DynamicEncoder.calc_loss,
                    self.gcns[i],
                    zs[start : end],
                    partition, nratio
                )
            )
            start = end 

        tot_loss = torch.zeros(1)
        for f in futs:
            tot_loss += f.wait()

        return [tot_loss.true_divide(sum(self.len_from_each))]

    
    def score_edges(self, zs, partition, nratio=1):
        '''
        Gets edge scores from dist modules, and negative edges
        Same deal, just add the padding to the zs

        zs : torch.Tensor 
            A T x d x N tensor of node embeddings generated by each graph snapshot
            Need to offset according to how far in the future embeddings are supposed
            to represent.
        partition : int
            enum representing train, validation, test sent to workers
        nratio : float
            The workers sample nratio * |E| negative edges for calculating loss
        '''

        zs = torch.cat(
            [torch.zeros(zs[0].size()).unsqueeze(0), zs]
        )
        
        futs = []
        start = 0
    
        for i in range(self.num_workers):
            end = start + self.len_from_each[i]
            futs.append(
                _remote_method_async(
                    DynamicEncoder.score_edges,
                    self.gcns[i],
                    zs[start : end], 
                    partition, nratio
                )
            )
            start = end 

        pos, neg = zip(*[f.wait() for f in futs])
        return torch.cat(pos, dim=0), torch.cat(neg, dim=0)