from copy import deepcopy

import torch 
from torch import nn 
from torch.autograd import Variable
from torch.distributed import rpc
from torch.nn.parallel import DistributedDataParallel as DDP
from torch_geometric.nn import MessagePassing
from zayas_graph_modules.samplers import SampleMean

from .embedders import GCN 
from .framework import Euler_Encoder
from .static import StaticEncoder, StaticRecurrent
from .utils import _remote_method, _remote_method_async, _decoder_rrefs


# From Ramesh's paper, this is f(u,v,N(u))
class TEdgeAnoms(nn.Module):
    def __init__(self, embed_dim, num_nodes, n_samples):
        super().__init__()

        self.H = SampleMean(n_samples=n_samples)
        self.W = nn.Linear(embed_dim, num_nodes)
        self.sm = nn.Softmax(dim=1)
        
        # Computes a softmax on inputs, so sm is only used 
        # for scoring
        self.ce_loss = nn.CrossEntropyLoss()

    def forward(self, x, ei, no_grad=False):
        if no_grad:
            with torch.no_grad():
                return self.inner_forward(x, ei)
        return self.inner_forward(x, ei)

    def inner_forward(self, x, ei):
        H = self.H(x,ei)
        distro = self.W(H)

        src, dst = ei 
        return self.ce_loss(
            distro[src],
            dst 
        ), H

    def score(self, H, ei):
        '''
        Scores all edges in ei given a precalculated
        zs matrix of static (possibly made w missing edges)

        Only called in eval so never need grads
        '''
        with torch.no_grad():
            distros = self.sm(self.W(H))
            src,dst = ei

            src_score = distros[src, dst]
            dst_score = distros[dst, src]

            return (src_score+dst_score)*0.5

def tedge_rref(loader, kwargs, h_dim, z_dim, **kws):
    return TEdgeEncoder(
        GCN(loader, kwargs, h_dim, z_dim),
        TEdgeAnoms, z_dim 
    )

class TEdgeDecoder(DDP):
    '''
    Extended DDP for TEdgeAnom
    '''
    def score(self, H, ei):
        return self.module.score(H, ei)

class TEdgeEncoder(StaticEncoder):
    def __init__(self, encoder, decoder_constructor, z_dim, **kwargs):
        super().__init__(encoder, **kwargs)
        
        # Cleaner naming convention
        self.encoder = self.module      

        # Need to wrap in a DDP to coordinate params again. Luckilly this time it's pretty
        # simple to do. We just have to add one additional method to the baseline DDP class
        self.decoder = TEdgeDecoder(
            decoder_constructor(
                z_dim//2, 
                self.module.data.num_nodes, 
                5
            )
        )

    def parameters(self, recurse: bool = True):
        '''
        Exclude decoder params as those are trained seperately
        '''
        return self.encoder.parameters(recurse=recurse)

    def decoder_parameters(self, recurse: bool=True):
        '''
        Exclude encoder params
        ''' 
        return self.decoder.parameters(recurse=recurse)

    def anom_forward(self, zs, partition, no_grad):
        loss = torch.zeros((1))
        hs = []
        
        for i in range(self.encoder.data.T):
            ei = self.encoder.data.ei_masked(partition, i)
            if ei.size(1):
                l, h = self.decoder(zs[i], ei, no_grad=no_grad)
            
                loss += l
                hs.append(h)

        return loss.true_divide(len(zs)), torch.stack(hs)

    def decode_all(self, zs, unsqueeze=True):
        '''
        Given node embeddings, return edge likelihoods for 
        all subgraphs held by this model
        For static model, it's very simple. Just return the embeddings
        for ei[n] given zs[n]

        zs : torch.Tensor
            A T x d x N tensor of node embeddings generated by the models, 
            it is safe to assume z[n] are the embeddings for nodes in the 
            snapshot held by this model's TGraph at timestep n
        '''
        assert not zs.size(0) < self.encoder.data.T, \
            "%s was given fewer embeddings than it has time slices"\
            % rpc.get_worker_info().name

        assert not zs.size(0) > self.encoder.data.T, \
            "%s was given more embeddings than it has time slices"\
            % rpc.get_worker_info().name

        preds = []
        ys = []
        cnts = []
        for i in range(self.encoder.data.T):
            preds.append(
                self.decoder.score(zs[i], self.encoder.data.eis[i])
            )    
            ys.append(self.encoder.data.ys[i])
            cnts.append(self.encoder.data.cnt[i])

        return preds, ys, cnts

    def score_edges(self, z, partition, nratio, zscores=False):
        '''
        Given a set of Z embeddings, returns likelihood scores for all known
        edges, and randomly sampled negative edges

        z : torch.Tensor
            A T x d x N tensor of node embeddings generated by the models, 
            it is safe to assume z[n] are the embeddings for nodes in the 
            snapshot held by this model's TGraph at timestep n
        partition : int 
            An enum representing if this is training/validation/testing for 
            generating negative edges 
        nratio : float
            The model samples nratio * |E| negative edges for calculating loss
        '''
        if zscores:
            return super().score_edges(z, partition, nratio)

        n = self.encoder.data.get_negative_edges(partition, nratio)

        p_scores = []
        n_scores = []

        for i in range(len(z)):
            p = self.encoder.data.ei_masked(partition, i)
            if p.size(1) == 0:
                continue

            p_scores.append(self.decoder.score(z[i], p))
            n_scores.append(self.decoder.score(z[i], n[i]))

        p_scores = torch.cat(p_scores, dim=0)
        n_scores = torch.cat(n_scores, dim=0)

        return p_scores, n_scores


class TEdgeRecurrent(StaticRecurrent):
    def __init__(self, rnn: nn.Module, remote_rrefs: list):
        super().__init__(rnn, remote_rrefs)
        self.decoding = False 

    def anom_forward(self, mask_enum, zs, no_grad=False):
        # Train the anomaly detector on the output of the embedder at the same time 
        # note the Variable though; loss here won't backprop into the GNN/RNN
        self.decoding = True
        
        futs = []
        start = 0

        # Called the first time to get Z stored on the workers
        zs = Variable(zs)
        for i in range(self.num_workers):
            end = start + self.len_from_each[i]
            futs.append(
                _remote_method_async(
                    TEdgeEncoder.anom_forward,
                    self.gcns[i],
                    zs[start : end],
                    mask_enum,
                    no_grad
                )
            )
            start = end 

        # Workers just run loss and return it
        loss, hs = zip(*[f.wait() for f in futs])
        
        self.H = hs
        return loss 

    def score_all(self, zs, zscores=False):
        '''
        Has the distributed models score and label all of their edges
        Sends workers embeddings such that H[n] is used to reconstruct graph at 
        snapshot n

        zs : torch.Tensor 
            |T| x |N| x d tensor of embeddings generated by Euler model 
            Not assumed to be on workers already as this method is only ever called
            during evaluation.
        '''
        if zscores:
            # Uses StaticEncoder.score_all method (i.e. just uses inner product)
            return super().score_all(zs)

        futs = [
            _remote_method_async(
                TEdgeEncoder.decode_all,
                self.gcns[i],
                self.H[i]
            )
            for i in range(self.num_workers)
        ]

        obj = [f.wait() for f in futs]
        scores, ys, cnts = zip(*obj)
        
        # Compress into single list of snapshots
        scores = sum(scores, [])
        ys = sum(ys, [])
        cnts = sum(cnts, [])

        return scores, ys, cnts        

    def score_edges(self, zs, partition, nratio=1, zscore=False):
        if (not self.decoding) or zscore:
            return super().score_edges(zs, partition, nratio)

        # Start using detectors after they've been trained. This is called to 
        # tune the threshold during validation    
        futs = []
        start = 0
        for i in range(self.num_workers):
            end = start + self.len_from_each[i]
            futs.append(
                _remote_method_async(
                    TEdgeEncoder.score_edges,
                    self.gcns[i],
                    self.H[i], partition, nratio
                )
            )
            start = end 

        pos, neg = zip(*[f.wait() for f in futs])
        return torch.cat(pos, dim=0), torch.cat(neg, dim=0)

    def decoder_parameter_rrefs(self):
        '''
        Distributed optimizer needs RRefs to params rather than the literal
        locations of them that you'd get with self.parameters(). This returns
        a parameter list of all remote workers and an RRef of the RNN held by
        the recurrent layer
        '''
        params = []
        for rref in self.gcns: 
            params.extend(
                _remote_method(
                    _decoder_rrefs, rref
                )
            )

        return params

   
    