from copy import deepcopy

import torch 
from torch import nn 
from torch.nn.parallel import DistributedDataParallel as DDP

from .framework import Euler_Embed_Unit, Euler_Encoder, Euler_Recurrent
from .tedge import TEdgeAnoms
from .utils import *

class SoftmaxDetector(Euler_Embed_Unit):
    '''
    Essentially a wrapper around the TEdgeAnoms class that
    holds data locally. Meant to be held in a DDP instance
    '''
    def __init__(self, data_load, data_kws, embeds, n_samples=5):
        super().__init__(data_load, data_kws)
        
        self.data.set_x(embeds)
        self.anoms = TEdgeAnoms(self.data.x_dim, self.data.num_nodes, n_samples)


    def forward(self, mask_enum, no_grad):
        hs, loss = [], torch.zeros((1))

        for i in range(self.data.T):
            ei = self.data.ei_masked(mask_enum, i)
            z = self.data.xs[i]

            h, loss = self.anoms(z, ei, no_grad)

        return torch.stack(hs), loss 

    def score(self, h, ei):
        return self.anoms.score(h, ei)


class DistributedSoftmaxDetector(Euler_Encoder):
    def calc_loss(self, z, partition, nratio):
        '''
        Loss is calculated and returned during forward pass
        '''
        pass 

    
    def decode_all(self, hs, unsqueeze=False):
        '''
        Given node embeddings, return edge likelihoods for all edges in snapshots held by this model. 
        Implimented differently for predictive and static models

        hs : torch.Tensor
            A T x d x N tensor of node embeddings generated by the models, 
            it is safe to assume z[n] are the embeddings for nodes in the 
            snapshot held by this model's TGraph at timestep n

        returns: 
            tuple of 
                [Tensor, ...] scores at each snapshot
                [Tensor, ...] labels for each snapshot
                [Tensor, ...] number of edge repetitions at each snapshot
        '''

        scores = []
        for i in range(self.module.data.T):
            h, ei = hs[i], self.module.data.eis[i]
            
            scores.append(
                self.module.score(
                    h, ei
                )
            )

        return scores, self.module.data.ys, self.module.data.cnts
            
    
    def score_edges(self, hs, partition, nratio):
        '''
        Scores all known edges and randomly sampled non-edges. 
        Generates validation results for training

        z : torch.Tensor
            A T x d x N tensor of node embeddings generated by the models, 
            it is safe to assume z[n] are the embeddings for nodes in the 
            snapshot held by this model's TGraph at timestep n
        partition : int 
            An enum representing if this is training/validation/testing for 
            generating negative edges 
        nratio : float
            The model samples nratio * |E| negative edges for calculating loss
        '''
        n = self.module.data.get_negative_edges(partition, nratio)

        p_scores = []
        n_scores = []

        for i in range(self.module.data.T):
            p = self.module.data.ei_masked(partition, i)
            if p.size(1) == 0:
                continue

            p_scores.append(self.module.score(hs[i], p))
            n_scores.append(self.module.score(hs[i], n[i]))

        p_scores = torch.cat(p_scores, dim=0)
        n_scores = torch.cat(n_scores, dim=0)

        return p_scores, n_scores


def softmax_detector_rref(data_load, data_kws, embeds, n_samples=5):
    return DistributedSoftmaxDetector(
        SoftmaxDetector(data_load, data_kws, embeds, n_samples=n_samples)
    )


class SoftmaxDetectorCoordinator(nn.Module):
    def __init__(self, remote_rrefs: list):
        super(SoftmaxDetectorCoordinator, self).__init__()

        self.detectors = remote_rrefs
        self.num_workers = len(self.detectors)
        self.len_from_each = []

        # Used for LR when classifying anomalies
        self.cutoff = 0.5

    def forward(self, partition):
        

    '''
    Internal methods all need to be updated to reflect lack of inner RNN
    '''
    def parameter_rrefs(self):
        '''
        Distributed optimizer needs RRefs to params rather than the literal
        locations of them that you'd get with self.parameters(). This returns
        a parameter list of all remote workers and an RRef of the RNN held by
        the recurrent layer
        '''
        params = []
        for rref in self.detectors: 
            params.extend(
                _remote_method(
                    _param_rrefs, rref
                )
            )
        
        return params

   
    def save_states(self):
        '''
        Makes a copy of the current state dict in the 
        distributed detectors (just worker 0)
        '''
        detector = _remote_method(
            DDP.state_dict, self.detectors[0]
        )

        return detector, deepcopy(self.state_dict())

    
    def load_states(self, state_dict):
        '''
        Given the state dict for one detector, load it
        into the dist models

        state_dict : dict  
            Parameter dict for remote worker 
        '''
        jobs = []
        for rref in self.detectors:
            jobs.append(
                _remote_method_async(
                    DDP.load_state_dict, rref, 
                    state_dict
                )
            )

        [j.wait() for j in jobs]

    
    def train(self, mode=True):
        '''
        Propogate training mode to all workers
        ''' 
        super(SoftmaxDetectorCoordinator, self).train(True)
        [_remote_method(
            Euler_Encoder.train,
            self.detectors[i],
            mode=mode
        ) for i in range(self.num_workers)]


    def eval(self):
        '''
        Propogate training mode to all workers
        '''
        super(SoftmaxDetectorCoordinator, self).train(False)
        [_remote_method(
            Euler_Encoder.train,
            self.detectors[i],
            mode=False
        ) for i in range(self.num_workers)]